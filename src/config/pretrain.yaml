trainer: 
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${utils.output_dir}
  min_epochs: 1
  max_epochs: 250
  deterministic: True
  devices: [5,6,7]

dataset:
  _target_: __main__.PretrainingDataset
  config:
    datadir: /storage/chandak/Silver/data
    outpatient_lvef_only: False
    ecg: 
      storedir: /storage/shared/ecg/mgh
      channels: ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
      wavelet: False
  split: None 

model: 
  _target_:  __main__.PCLR
  config: 
    ecg_repr_dim: 256 # currently redunant 
    dropout_prob: 0
  objective: 
    _target_: lightly.loss.NTXentLoss
    temperature: 0.1
    memory_bank_size: 0
    gather_distributed: True

utils:
  seed: 140799
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}

wandb:
  _target_: lightning.pytorch.loggers.WandbLogger
  project: ecg
  entity: payalchandak
  name: 'pretrain'
  resume: 'allow'
  log_model: True

optimizer: 
  batch_size: 1024
  num_dataloader_workers: 5
  init_lr: 1e-3 # 1e-6 to 1e-2
  betas: # combinations (0.9, 0.999) (0.9, 0.99) (0.95, 0.999)
    - 0.9 # 0.8 to 0.999
    - 0.99 # 0.99 to 0.9999
  weight_decay: 0
  end_lr_frac_of_init_lr: 1e-3 # 1e-4 to 0.9
  lr_frac_warmup_steps: 0.01 # 1e-6 to 5e-1
  lr_decay_power: 1.0 # 0.5 to 5.0
  num_warmup_steps: ??? 
  num_training_steps: ??? 
  end_lr: ??? 

callbacks: 
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: train/loss
    mode: min
    dirpath: '${utils.output_dir}/checkpoints' 
    filename: 'epoch_{epoch:03d}'
    auto_insert_metric_name: False
    save_last: True,
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step