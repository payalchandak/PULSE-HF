{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T17:30:02.027188Z",
     "start_time": "2025-01-19T17:29:59.408942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandak/miniconda3/envs/ecg/lib/python3.8/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import lightning as L\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import hydra\n",
    "import wandb\n",
    "from dataset import SupervisedDataset\n",
    "from lightning_modules import SupervisedTask\n",
    "from models.ecg_models import *\n",
    "from run import interpolate\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "MY_NAVY = '#001F54'\n",
    "COLOR_BELOW = 'firebrick'\n",
    "COLOR_ABOVE = 'gold'\n",
    "LINE_BELOW = '--'\n",
    "LINE_ABOVE = '-.'\n",
    "LABEL_ALL='Any LVEF'\n",
    "LABEL_ABOVE='Last LVEF > 40%'\n",
    "LABEL_BELOW='Last LVEF < 40%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographics \n",
    "df = pl.read_parquet('/storage2/payal/Dropbox (Partners HealthCare)/private/SILVER/data/data.parquet')\n",
    "df = df.filter(pl.col('split')!='external')\n",
    "\n",
    "for c in [\n",
    " 'diabetes_mellitus',\n",
    " 'hypertension',\n",
    " 'atherosclerosis',\n",
    " 'chronic_obstructive_pulmonary_disease',\n",
    " 'atrial_fibrillation',\n",
    " 'slgt2',\n",
    " 'angio',\n",
    " 'betablocker',\n",
    " 'mra',\n",
    " 'diuretic',\n",
    "]:\n",
    "    print(c, round(100* df.select(['empi',c]).group_by('empi').sum().filter(pl.col(c)>0).unique('empi').height / df.unique('empi').height, 1), '%')\n",
    "\n",
    "\n",
    "dem = pl.read_parquet(\n",
    "    '/storage2/payal/dropbox/private/data/processed/demographic.parquet'\n",
    ").join(\n",
    "    df, on='empi', how='inner'\n",
    ").with_columns(\n",
    "    ((pl.col('ecg_date')-pl.col('date_of_birth')).cast(pl.Duration).dt.total_days()/365.25).alias('age')\n",
    ")\n",
    "# \n",
    "# dem.select(['empi','age']).mean(), dem.select(['empi','age']).std()\n",
    "dem.unique('empi').group_by('sex').len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 140799\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "device = 'cuda:1'\n",
    "cfg = OmegaConf.create(wandb.Api().run(\"payal-collabs/SILVER/f7apd99r\").config)\n",
    "L.seed_everything(cfg.utils.seed)\n",
    "train_pyd = hydra.utils.instantiate(cfg.dataset, split='train')\n",
    "cfg = interpolate(cfg, train_pyd)\n",
    "del train_pyd\n",
    "trainer = L.Trainer(devices=[int(device[-1])]) \n",
    "LM = SupervisedTask.load_from_checkpoint(cfg.best_model_path, map_location=torch.device(device))\n",
    "model = LM.model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "cfg.optimizer.batch_size = 128\n",
    "cfg.dataset.config.label = 'future_1_365_any_below_40'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test' # test, external\n",
    "if split == 'mimic': \n",
    "    cfg.dataset.config.datadir = '/storage/shared/mimic/'\n",
    "    cfg.dataset.config.ecg.storedir = '/storage/shared/mimic/raw/ecg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/storage2/payal/Dropbox (Partners HealthCare)/private/SILVER/src/dataset.py:28\u001b[0m, in \u001b[0;36mSupervisedDataset.get_ecg\u001b[0;34m(self, pth, date, wavelet, samples, convert_to_mV)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m---> 28\u001b[0m     \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mstr\u001b[39m(date)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m: \n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.8/site-packages/h5py/_hl/files.py:542\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m track_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.8/site-packages/h5py/_hl/compat.py:19\u001b[0m, in \u001b[0;36mfilename_encode\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mEncode filename for use in the HDF5 library.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mfilenames in h5py for more information.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not dict",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m pyd\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pyd\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pyd)\n\u001b[1;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m----> 6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpyd\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      7\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m      8\u001b[0m     num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m      9\u001b[0m     collate_fn \u001b[38;5;241m=\u001b[39m pyd\u001b[38;5;241m.\u001b[39mcollate,\n\u001b[1;32m     10\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m out \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(LM, loader)\n\u001b[1;32m     14\u001b[0m pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m out]))\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/storage2/payal/Dropbox (Partners HealthCare)/private/SILVER/src/dataset.py:57\u001b[0m, in \u001b[0;36mSupervisedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m item\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item[k] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: item[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 57\u001b[0m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ecg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mecg_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mecg_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwavelet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavelet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m/storage2/payal/Dropbox (Partners HealthCare)/private/SILVER/src/dataset.py:30\u001b[0m, in \u001b[0;36mSupervisedDataset.get_ecg\u001b[0;34m(self, pth, date, wavelet, samples, convert_to_mV)\u001b[0m\n\u001b[1;32m     28\u001b[0m         h5py\u001b[38;5;241m.\u001b[39mFile(pth, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mstr\u001b[39m(date)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m: \n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(), date, \u001b[38;5;28mstr\u001b[39m(date)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     31\u001b[0m     f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(pth, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mstr\u001b[39m(date)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     33\u001b[0m signal \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.8/site-packages/h5py/_hl/files.py:542\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    540\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(name)\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mASCII\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m track_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m     track_order \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mget_config()\u001b[38;5;241m.\u001b[39mtrack_order\n",
      "File \u001b[0;32m~/miniconda3/envs/ecg/lib/python3.8/site-packages/h5py/_hl/compat.py:19\u001b[0m, in \u001b[0;36mfilename_encode\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilename_encode\u001b[39m(filename):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    Encode filename for use in the HDF5 library.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    filenames in h5py for more information.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not dict"
     ]
    }
   ],
   "source": [
    "# predictions \n",
    "pyd = hydra.utils.instantiate(cfg.dataset, split=split)\n",
    "pyd.data = pyd.data.reset_index(drop=1)\n",
    "assert len(pyd)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset = pyd[:5],\n",
    "    batch_size = cfg.optimizer.batch_size,\n",
    "    num_workers = 0, \n",
    "    collate_fn = pyd.collate,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "out = trainer.predict(LM, loader)\n",
    "pred = np.array(torch.sigmoid(torch.cat([x[0] for x in out])).tolist())\n",
    "true = np.array(torch.cat([x[1] for x in out]).tolist())\n",
    "idx_below = pyd.data.query('tag_hfref').index.values\n",
    "idx_above = pyd.data.query('~tag_hfref').index.values\n",
    "idx_no_com = pyd.data.query('~tag_hfref').query('hypertension==0').query('diabetes_mellitus==0').query('atherosclerosis==0').query('chronic_obstructive_pulmonary_disease==0').query('atrial_fibrillation==0').index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_no_med = pyd.data.query('~tag_hfref').query('angio==0').query('betablocker==0').query('mra==0').query('diuretic==0').index.values\n",
    "idx_healthy = pyd.data.query('~tag_hfref').query('angio==0').query('betablocker==0').query('mra==0').query('diuretic==0').query('hypertension==0').query('diabetes_mellitus==0').query('atherosclerosis==0').query('chronic_obstructive_pulmonary_disease==0').query('atrial_fibrillation==0').index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap test \n",
    "pyd = hydra.utils.instantiate(cfg.dataset, split=split)\n",
    "pyd.data = pyd.data.reset_index(drop=1)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset = pyd,\n",
    "    batch_size = 2,\n",
    "    num_workers = 0, \n",
    "    collate_fn = pyd.collate,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "def move_to_device(batch, device):\n",
    "    if isinstance(batch, dict):  # If batch is a dictionary\n",
    "        return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "    elif isinstance(batch, (list, tuple)):  # If batch is a list or tuple\n",
    "        return type(batch)(v.to(device) if isinstance(v, torch.Tensor) else v for v in batch)\n",
    "    elif isinstance(batch, torch.Tensor):  # If batch is a tensor\n",
    "        return batch.to(device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported batch type: {type(batch)}\")\n",
    "\n",
    "for batch in loader: \n",
    "    batch = move_to_device(batch, device)\n",
    "    break\n",
    "\n",
    "import shap\n",
    "class ShapModule(torch.nn.Module): \n",
    "    def __init__(self, model, device): \n",
    "        super(ShapModule, self).__init__()\n",
    "        self.model = model \n",
    "        self.device = device \n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = self.model.forward(x)\n",
    "        outputs = outputs['loss'].unsqueeze(0).unsqueeze(1)\n",
    "        return outputs\n",
    "shap_model = ShapModule(model, device)\n",
    "explainer = shap.DeepExplainer(model=shap_model, data=[batch])\n",
    "explainer.shap_values([batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saliency \n",
    "def move_to_device(batch, device):\n",
    "    if isinstance(batch, dict):  # If batch is a dictionary\n",
    "        return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "    elif isinstance(batch, (list, tuple)):  # If batch is a list or tuple\n",
    "        return type(batch)(v.to(device) if isinstance(v, torch.Tensor) else v for v in batch)\n",
    "    elif isinstance(batch, torch.Tensor):  # If batch is a tensor\n",
    "        return batch.to(device)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported batch type: {type(batch)}\")\n",
    "\n",
    "def plot_saliency(batch, idx=0):\n",
    "    batch = {k:v[idx:idx+2] for (k,v) in batch.items()}\n",
    "    batch['ecg'].requires_grad_()\n",
    "    output = model(batch)\n",
    "    model.zero_grad()\n",
    "    output['loss'].backward()\n",
    "    saliency = batch['ecg'].grad.abs()\n",
    "\n",
    "    ecg = batch['ecg'][0].detach().cpu().numpy()  \n",
    "    saliency = saliency[0].detach().cpu().numpy()  \n",
    "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
    "    num_leads = ecg.shape[0]  \n",
    "    time_steps = ecg.shape[1]  \n",
    "    fig, axes = plt.subplots(12, 1, figsize=(15, 20), sharex=True)\n",
    "    for lead_idx in range(12):\n",
    "        ax = axes[lead_idx]\n",
    "        time = np.arange(time_steps)\n",
    "        for i in range(time_steps - 1):\n",
    "            ax.plot( time[i:i+2], ecg[lead_idx, i:i+2], color='gray', linewidth=1)\n",
    "        for val in [0, 0.25, 0.5, 0.75]:\n",
    "            c_idx = np.argwhere(saliency[lead_idx]>val)\n",
    "            ax.scatter(time[c_idx], ecg[lead_idx][c_idx], c=saliency[lead_idx][c_idx], cmap='Blues', s=100, alpha=0.2*val)\n",
    "            ax.scatter(time[c_idx], ecg[lead_idx][c_idx], c=saliency[lead_idx][c_idx], cmap='Blues', s=50, alpha=0.7*val)\n",
    "        ax.set_title(f\"Lead {lead_idx + 1}\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "    axes[-1].set_xlabel(\"Time Steps\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for batch in loader: \n",
    "    pos_mask = batch['label']==1\n",
    "    batch = {k:v[pos_mask] for (k,v) in batch.items()}\n",
    "    batch = move_to_device(batch, device)\n",
    "    plot_saliency(batch, np.random.choice(pos_mask.sum().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration \n",
    "\n",
    "strategy = 'uniform' # 'quantile'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='-', color='silver', alpha=0.5)\n",
    "\n",
    "for idx, lst, lbl in [(None, '-', LABEL_ALL), (idx_above, LINE_ABOVE, LABEL_ABOVE), (idx_below, LINE_BELOW, LABEL_BELOW)]:\n",
    "    \n",
    "    true_here = true[idx].reshape(-1)\n",
    "    pred_here = pred[idx].reshape(-1)\n",
    "\n",
    "    ci_percentile = 95\n",
    "    bootstrap_results = []\n",
    "    n_samples = len(true_here)\n",
    "    for _ in range(1000):\n",
    "        indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n",
    "        true_sample = true_here[indices]\n",
    "        pred_sample = pred_here[indices]\n",
    "        prob_true, prob_pred = calibration_curve(true_sample, pred_sample, strategy=strategy, n_bins=10)\n",
    "        bootstrap_results.append(prob_true)\n",
    "    lower_bound = np.percentile(bootstrap_results, (100 - ci_percentile) / 2, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_results, 100 - (100 - ci_percentile) / 2, axis=0)\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(true_here, pred_here, strategy=strategy, n_bins=10)\n",
    "\n",
    "    # plt.fill_between(prob_pred, lower_bound, upper_bound, color=MY_NAVY, alpha=0.4, linewidth=0.5, interpolate=False)\n",
    "    # plt.plot(prob_pred, prob_true, marker='o', color=MY_NAVY, linestyle=lst, label='Calibration curve')\n",
    "    plt.errorbar(\n",
    "        prob_pred,\n",
    "        prob_true,\n",
    "        yerr=np.abs(np.vstack([lower_bound-prob_true, upper_bound-prob_true])),\n",
    "        capsize=3,\n",
    "        marker='o',\n",
    "        markersize=3,\n",
    "        linestyle=lst,\n",
    "        linewidth=1,\n",
    "        color=MY_NAVY,\n",
    "        label=lbl,\n",
    "    )\n",
    "    \n",
    "plt.xlabel('Mean predicted probability', color='gray')\n",
    "plt.ylabel('Fraction of positives', color='gray')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = [h[0] for h in handles]\n",
    "ax.legend(handles, labels, markerscale=0.001)\n",
    "ax.tick_params(axis='y', colors='gray')\n",
    "ax.spines['left'].set_color('gray')\n",
    "ax.spines['left'].set_linewidth(1)\n",
    "ax.tick_params(axis='x', colors='gray', which='both')\n",
    "ax.spines['bottom'].set_color('gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hosmer lemeshow test\n",
    "def hosmer_lemeshow_test(y_true, y_pred_probs, g=10):\n",
    "    \"\"\"\n",
    "    Perform the Hosmer–Lemeshow test for goodness-of-fit.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary outcomes (0 or 1).\n",
    "    y_pred_probs (array-like): Predicted probabilities from the model.\n",
    "    g (int): Number of groups to divide the data into (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    hl_stat (float): Hosmer–Lemeshow test statistic.\n",
    "    p_value (float): Corresponding p-value.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with actual and predicted values\n",
    "    data = pd.DataFrame({'y_true': y_true, 'y_pred_probs': y_pred_probs})\n",
    "\n",
    "    # Create quantile-based bins\n",
    "    data['bin'] = pd.qcut(data['y_pred_probs'], q=g, duplicates='drop')\n",
    "\n",
    "    # Group data by bins\n",
    "    grouped = data.groupby('bin')\n",
    "\n",
    "    # Calculate observed and expected frequencies\n",
    "    obs_freq = grouped['y_true'].sum()\n",
    "    exp_freq = grouped['y_pred_probs'].sum()\n",
    "    n = grouped.size()\n",
    "    p = exp_freq / n\n",
    "\n",
    "    # Avoid division by zero\n",
    "    temp = p * (1 - p)\n",
    "    temp[temp == 0] = 1e-10\n",
    "\n",
    "    # Calculate the Hosmer–Lemeshow statistic\n",
    "    hl_stat = np.sum(((obs_freq - exp_freq) ** 2) / (n * temp))\n",
    "\n",
    "    # Degrees of freedom\n",
    "    dof = g - 2\n",
    "\n",
    "    # Calculate p-value\n",
    "    p_value = 1 - chi2.cdf(hl_stat, dof)\n",
    "\n",
    "    return hl_stat, p_value\n",
    "\n",
    "hl_stat, p_value = hosmer_lemeshow_test(true, pred)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(true, pred), roc_auc_score(true[idx_above], pred[idx_above]), roc_auc_score(true[idx_no_com], pred[idx_no_com]), roc_auc_score(true[idx_no_med], pred[idx_no_med]), roc_auc_score(true[idx_healthy], pred[idx_healthy]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auroc \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true, pred)\n",
    "sens = tpr\n",
    "spec = 1-fpr\n",
    "\n",
    "fpr_below, tpr_below, thresholds_below = roc_curve(true[idx_below], pred[idx_below])\n",
    "sens_below = tpr_below\n",
    "fpr_above, tpr_above, thresholds_above = roc_curve(true[idx_above], pred[idx_above])\n",
    "sens_above = tpr_above\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=MY_NAVY, label=LABEL_ALL)\n",
    "plt.plot(fpr_above, tpr_above, color=MY_NAVY, linestyle=LINE_ABOVE, label=LABEL_ABOVE)\n",
    "plt.plot(fpr_below, tpr_below, color=MY_NAVY, linestyle=LINE_BELOW, label=LABEL_BELOW)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"silver\")\n",
    "plt.xlabel(\"False Positive Rate\", color=\"gray\")\n",
    "plt.ylabel(\"True Positive Rate\", color=\"gray\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.0005)\n",
    "plt.gca().tick_params(axis='y', colors='gray')\n",
    "plt.gca().spines['left'].set_color('gray')\n",
    "plt.gca().spines['left'].set_linewidth(1)\n",
    "plt.gca().tick_params(axis='x', colors='gray', which='both')\n",
    "plt.gca().spines['bottom'].set_color('gray')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.grid(visible=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary sensitivity to thresholds \n",
    "dct = {}\n",
    "for i in np.concatenate([np.arange(0.1, 1, 0.1), np.arange(0.8, 1.00001, 0.01)]):\n",
    "    i = round(i, 2)\n",
    "    for j in range(6,0,-1):\n",
    "        idx = np.where(np.round(spec, j) == i)[0]\n",
    "        if idx.any(): \n",
    "            break\n",
    "    dct[i] = thresholds[idx]\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute specificity, ppv, npv with bootstrap \n",
    "\n",
    "def compute_metrics_bootstrap(true, pred_probs, prevalence, num_bootstrap=1000, confidence=0.95):\n",
    "\n",
    "    _, sens_here, thresholds_here = roc_curve(true, pred_probs)\n",
    "    select_thresholds = []\n",
    "    for i in np.arange(0.1, 1, 0.1): # sensitivity\n",
    "        i = round(i, 2)\n",
    "        for j in range(6,0,-1):\n",
    "            idx = np.where(np.round(sens_here, j) == i)[0]\n",
    "            if idx.any(): \n",
    "                break\n",
    "        select_thresholds.append(thresholds_here[idx][0])\n",
    "\n",
    "    prevalence = prevalence / 100\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    ppv_list = []\n",
    "    npv_list = []\n",
    "\n",
    "    specificity_err = []\n",
    "    ppv_err = []\n",
    "    npv_err = []\n",
    "\n",
    "    for threshold in select_thresholds:\n",
    "        samples_specificity = []\n",
    "        samples_ppv = []\n",
    "        samples_npv = []\n",
    "\n",
    "        for _ in range(num_bootstrap):\n",
    "            idx = np.random.choice(len(true), len(true), replace=True)\n",
    "            true_sample = true[idx]\n",
    "            pred_probs_sample = pred_probs[idx]\n",
    "            \n",
    "            pred_labels = (pred_probs_sample >= threshold).astype(int)\n",
    "            # tn, fp, fn, tp = confusion_matrix(true_sample, pred_labels).ravel()\n",
    "            tn = np.sum((pred_labels == 0) & (true_sample == 0))\n",
    "            fp = np.sum((pred_labels == 1) & (true_sample == 0))\n",
    "            fn = np.sum((pred_labels == 0) & (true_sample == 1))\n",
    "            tp = np.sum((pred_labels == 1) & (true_sample == 1))\n",
    "\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            ppv = (sensitivity * prevalence) / (sensitivity * prevalence + (1 - specificity) * (1 - prevalence)) if (sensitivity * prevalence + (1 - specificity) * (1 - prevalence)) > 0 else 0\n",
    "            npv = (specificity * (1 - prevalence)) / (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence) if (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence) > 0 else 0\n",
    "\n",
    "            samples_specificity.append(specificity)\n",
    "            samples_ppv.append(ppv)\n",
    "            samples_npv.append(npv)\n",
    "\n",
    "        # Store mean metrics\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(np.percentile(samples_specificity, 50))\n",
    "        ppv_list.append(np.percentile(samples_ppv, 50))\n",
    "        npv_list.append(np.percentile(samples_npv, 50))\n",
    "\n",
    "        # Store confidence intervals\n",
    "        specificity_err.append((np.percentile(samples_specificity, 100 * (1 - confidence) / 2), np.percentile(samples_specificity, 100 * (confidence + (1 - confidence) / 2))))\n",
    "        ppv_err.append((np.percentile(samples_ppv, 100 * (1 - confidence) / 2), np.percentile(samples_ppv, 100 * (confidence + (1 - confidence) / 2))))\n",
    "        npv_err.append((np.percentile(samples_npv, 100 * (1 - confidence) / 2), np.percentile(samples_npv, 100 * (confidence + (1 - confidence) / 2))))\n",
    "\n",
    "    return (\n",
    "        np.array(sensitivity_list),\n",
    "        np.array(specificity_list),\n",
    "        np.array(ppv_list),\n",
    "        np.array(npv_list),\n",
    "        np.array(specificity_err),\n",
    "        np.array(ppv_err),\n",
    "        np.array(npv_err),\n",
    "    )\n",
    "\n",
    "prevalences = [10,15,20] # [5, 10, 20, 30, 40, 50]\n",
    "metrics = {}\n",
    "metrics_below = {}\n",
    "metrics_above = {}\n",
    "\n",
    "for p in prevalences: \n",
    "    metrics[p] = compute_metrics_bootstrap(true, pred, p)\n",
    "    metrics_below[p] = compute_metrics_bootstrap(true[idx_below].reshape(-1), pred[idx_below].reshape(-1), p)\n",
    "    metrics_above[p] = compute_metrics_bootstrap(1-true[idx_above].reshape(-1), 1-pred[idx_above].reshape(-1), p)\n",
    "    print(p)\n",
    "    # sensi, speci, ppv, npv, speci_err, ppv_err, npv_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specificity\n",
    "\n",
    "k = [k for k in metrics.keys()][0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sensi, speci, ppv, npv, speci_err, ppv_err, npv_err = metrics[k]\n",
    "plt.errorbar(\n",
    "    sensi,\n",
    "    speci,\n",
    "    yerr=np.abs(np.array(speci_err).T - speci),\n",
    "    capsize=3,\n",
    "    marker='o',\n",
    "    markersize=3,\n",
    "    linestyle='-',\n",
    "    linewidth=1,\n",
    "    color=MY_NAVY,\n",
    "    label='Any LVEF',\n",
    ")\n",
    "sensi, speci, ppv, npv, speci_err, ppv_err, npv_err = metrics_above[k]\n",
    "plt.errorbar(\n",
    "    sensi,\n",
    "    speci,\n",
    "    yerr=np.abs(np.array(speci_err).T - speci),\n",
    "    capsize=3,\n",
    "    marker='o',\n",
    "    markersize=3,\n",
    "    linestyle=LINE_ABOVE,\n",
    "    linewidth=1,\n",
    "    color=MY_NAVY,\n",
    "    label='Last LVEF > 40%',\n",
    ")\n",
    "sensi, speci, ppv, npv, speci_err, ppv_err, npv_err = metrics_below[k]\n",
    "plt.errorbar(\n",
    "    sensi,\n",
    "    speci,\n",
    "    yerr=np.abs(np.array(speci_err).T - speci),\n",
    "    capsize=3,\n",
    "    marker='o',\n",
    "    markersize=3,\n",
    "    linestyle=LINE_BELOW,\n",
    "    linewidth=1,\n",
    "    color=MY_NAVY,\n",
    "    label='Last LVEF < 40%',\n",
    ")\n",
    "plt.xlabel('Sensitivity', color='gray')\n",
    "plt.ylabel('Specificity', color='gray')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.005)\n",
    "plt.gca().tick_params(axis='y', colors='gray')\n",
    "plt.gca().spines['left'].set_color('gray')\n",
    "plt.gca().spines['left'].set_linewidth(1)\n",
    "plt.gca().tick_params(axis='x', colors='gray', which='both')\n",
    "plt.gca().spines['bottom'].set_color('gray')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = [h[0] for h in handles]\n",
    "ax.legend(handles, labels, markerscale=0.001)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppv, npv\n",
    "\n",
    "for mtr, lst, lbl in [(metrics, '-', LABEL_ALL), (metrics_above, LINE_ABOVE, LABEL_ABOVE), (metrics_below, LINE_BELOW, LABEL_BELOW)]:\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for prevalence in prevalences:\n",
    "        sensi, speci, ppv, npv, speci_err, ppv_err, npv_err = mtr[prevalence]\n",
    "        plt.errorbar(\n",
    "            sensi,\n",
    "            ppv,\n",
    "            yerr=np.abs(np.array(ppv_err).T - ppv),\n",
    "            label=f'{prevalence}% prevalence',\n",
    "            capsize=3,\n",
    "            marker='o',\n",
    "            markersize=3,\n",
    "            linestyle=lst,\n",
    "            linewidth=1,\n",
    "            color=MY_NAVY,\n",
    "        )\n",
    "        plt.text(\n",
    "            0.75, \n",
    "            ppv[7], \n",
    "            f'{prevalence}%', \n",
    "            fontsize=12, \n",
    "            color=MY_NAVY, \n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.1', edgecolor='none', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    plt.xlabel('Sensitivity', color='gray')\n",
    "    plt.ylabel('PPV', color='gray')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1.0005)\n",
    "    plt.gca().tick_params(axis='y', colors='gray')\n",
    "    plt.gca().spines['left'].set_color('gray')\n",
    "    plt.gca().spines['left'].set_linewidth(1)\n",
    "    plt.gca().tick_params(axis='x', colors='gray', which='both')\n",
    "    plt.gca().spines['bottom'].set_color('gray')\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for prevalence in prevalences:\n",
    "        sensi, speci, ppv, npv, speci_err, ppv_err, npv_err = mtr[prevalence]\n",
    "        plt.errorbar(\n",
    "            sensi,\n",
    "            npv,\n",
    "            yerr=np.abs(np.array(npv_err).T - npv),\n",
    "            capsize=3,\n",
    "            marker='o',\n",
    "            markersize=3,\n",
    "            linestyle=lst,\n",
    "            linewidth=1,\n",
    "            color=MY_NAVY,\n",
    "        )\n",
    "        plt.text(\n",
    "            0.25, \n",
    "            npv[2], \n",
    "            f'{prevalence}%', \n",
    "            fontsize=12, \n",
    "            color=MY_NAVY, \n",
    "            ha='center', \n",
    "            va='bottom',\n",
    "            bbox=dict(boxstyle='round,pad=0.1', edgecolor='none', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    plt.xlabel('Sensitivity', color='gray')\n",
    "    plt.ylabel('NPV', color='gray')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1.0005)\n",
    "    plt.gca().tick_params(axis='y', colors='gray')\n",
    "    plt.gca().spines['left'].set_color('gray')\n",
    "    plt.gca().spines['left'].set_linewidth(1)\n",
    "    plt.gca().tick_params(axis='x', colors='gray', which='both')\n",
    "    plt.gca().spines['bottom'].set_color('gray')\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensi, speci, ppv, npv, speci_err, ppv_err, npv_err = metrics[10]\n",
    "np.round(ppv,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppv(true_sample, pred_probs_sample): \n",
    "    for prevalence in [true_sample.sum()/len(true_sample)]:\n",
    "        for threshold in [0.57, 0.4, 0.19]:    \n",
    "            pred_labels = (pred_probs_sample >= threshold).astype(int)\n",
    "            # tn, fp, fn, tp = confusion_matrix(true_sample, pred_labels).ravel()\n",
    "            tn = np.sum((pred_labels == 0) & (true_sample == 0))\n",
    "            fp = np.sum((pred_labels == 1) & (true_sample == 0))\n",
    "            fn = np.sum((pred_labels == 0) & (true_sample == 1))\n",
    "            tp = np.sum((pred_labels == 1) & (true_sample == 1))\n",
    "\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            ppv = (sensitivity * prevalence) / (sensitivity * prevalence + (1 - specificity) * (1 - prevalence)) if (sensitivity * prevalence + (1 - specificity) * (1 - prevalence)) > 0 else 0\n",
    "            npv = (specificity * (1 - prevalence)) / (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence) if (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence) > 0 else 0\n",
    "\n",
    "            print(round(prevalence,2), round(sensitivity,2), round(ppv,2))\n",
    "        print()\n",
    "\n",
    "# overall ppv\n",
    "ppv(true, pred)\n",
    "\n",
    "# worsening ppv\n",
    "ppv(true[idx_above], pred[idx_above])\n",
    "\n",
    "# improving \n",
    "# ppv(1-true[idx_below], 1-pred[idx_below])\n",
    "\n",
    "\n",
    "\n",
    "ppv(true[idx_no_med], pred[idx_no_med])\n",
    "\n",
    "ppv(true[idx_healthy], pred[idx_healthy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred, true): \n",
    "    datapoints = len(true)\n",
    "    prevalence = sum(true)/len(true)\n",
    "    if prevalence>0 and prevalence<1: \n",
    "        auc = roc_auc_score(true, pred)\n",
    "        auc_95ci = bootstrap(pred=pred, true=true, metric_fn=roc_auc_score)\n",
    "    threshold = 0.4\n",
    "    pred_labels = (pred >= threshold).astype(int)\n",
    "    tn = np.sum((pred_labels == 0) & (true == 0))\n",
    "    fp = np.sum((pred_labels == 1) & (true == 0))\n",
    "    fn = np.sum((pred_labels == 0) & (true == 1))\n",
    "    tp = np.sum((pred_labels == 1) & (true == 1))\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = (sensitivity * prevalence) / (sensitivity * prevalence + (1 - specificity) * (1 - prevalence)) if (sensitivity * prevalence + (1 - specificity) * (1 - prevalence)) > 0 else 0\n",
    "    npv = (specificity * (1 - prevalence)) / (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence) if (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence) > 0 else 0\n",
    "    data = {\n",
    "        'datapoints':datapoints,\n",
    "        'prevalence':prevalence,\n",
    "        'auc':auc,\n",
    "        'auc_95ci':auc_95ci,\n",
    "        'sensitivity':sensitivity,\n",
    "        'specificity':specificity,\n",
    "        'ppv':ppv,\n",
    "        'npv':npv,\n",
    "    }\n",
    "    data = {k:round(v,3) for (k,v) in data.items()}\n",
    "    return data\n",
    "\n",
    "def bootstrap(pred, true, metric_fn, confidence=0.95, num_samples=1000):\n",
    "    lower_idx, upper_idx = int(num_samples*(1-confidence)/2), int(num_samples*(confidence)/2)\n",
    "    n = len(true)\n",
    "    samples = []\n",
    "    for _ in range(num_samples): \n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        samples.append( metric_fn(true[idx], pred[idx]) )\n",
    "    assert len(samples) == num_samples, f'failed to get {num_samples} samples for bootstrapping'\n",
    "    samples = sorted(samples)\n",
    "    return samples[upper_idx] - samples[lower_idx]\n",
    "\n",
    "def eval_queries(queries):\n",
    "    for q in queries: \n",
    "        if not q: idx = pyd.data.index.values\n",
    "        else: idx = pyd.data.query(q).index.values\n",
    "        x = eval(pred=1-pred[idx], true=1-true[idx])\n",
    "        print(f\"{q}:  \\n\\t{x['datapoints']} samples  \\n\\t{x['prevalence']*100}% prevalence  \\n\\t{x['auc']} ± {x['auc_95ci']}  \\n\\t{x['sensitivity']} sens  \\n\\t{x['specificity']} spec  \\n\\t{x['ppv']} ppv  \\n\\t{x['npv']} npv \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries([\n",
    "    '',\n",
    "    'paced==True',\n",
    "    'paced==False',\n",
    "    'transplant==0',\n",
    "    'transplant==1',\n",
    "    'hospitalisations==0',\n",
    "    'hospitalisations>=1',\n",
    "    'hospitalisations>=3',\n",
    "    'hospitalisations>=5',\n",
    "    'hospitalisations>=10',\n",
    "    'hospitalisations>=20',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries([\n",
    "    '~tag_hfref',\n",
    "    '~tag_hfref and diabetes_mellitus==0',\n",
    "    '~tag_hfref and diabetes_mellitus==1',\n",
    "    '~tag_hfref and hypertension==0',\n",
    "    '~tag_hfref and hypertension==1',\n",
    "    '~tag_hfref and atherosclerosis==0',\n",
    "    '~tag_hfref and atherosclerosis==1',\n",
    "    '~tag_hfref and chronic_obstructive_pulmonary_disease==0',\n",
    "    '~tag_hfref and chronic_obstructive_pulmonary_disease==1',\n",
    "    '~tag_hfref and atrial_fibrillation==0',\n",
    "    '~tag_hfref and atrial_fibrillation==1',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_queries([\n",
    "    'tag_hfref',\n",
    "    'tag_hfref and num_meds==0',\n",
    "    'tag_hfref and num_meds>0',\n",
    "    'tag_hfref and angio==0',\n",
    "    'tag_hfref and angio==1',\n",
    "    'tag_hfref and mra==0',\n",
    "    'tag_hfref and mra==1',\n",
    "    'tag_hfref and betablocker==0',\n",
    "    'tag_hfref and betablocker==1',\n",
    "    'tag_hfref and diuretic==0',\n",
    "    'tag_hfref and diuretic==1',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvef = pl.read_parquet('/storage2/payal/dropbox/private/data/processed/lvef.parquet')\n",
    "lvef = lvef.sort('lvef_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6112226064031894 #dct[0.7][0]\n",
    "pyd.data.loc[:,'true'] = true\n",
    "pyd.data.loc[:,'score'] = pred\n",
    "pyd.data.loc[:,'pred'] = (pred>(1-threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positives - early warning sign\n",
    "\n",
    "PER_PATIENT = False \n",
    "\n",
    "pyd_polars = pl.from_pandas(pyd.data).with_columns(\n",
    "    pl.when(\n",
    "        (pl.col('pred')!=pl.col('true')) # incorrect predictions \n",
    "        &\n",
    "        (pl.col('pred')==1) # predictions are that EF will fall\n",
    "    ).then(True).otherwise(False).alias('false_positive'),\n",
    "    pl.when(\n",
    "        (pl.col('pred')==pl.col('true')) # correct predictions \n",
    "        &\n",
    "        (pl.col('pred')==0) # predictions are that EF stay > 40\n",
    "    ).then(True).otherwise(False).alias('true_negative'),\n",
    ").filter(\n",
    "    ~pl.col('tag_hfref')\n",
    ")\n",
    "\n",
    "repeat_false_positives = []\n",
    "for group in pyd_polars.group_by(\"empi\"): \n",
    "    df = group[1].sort(['empi','ecg_date'])\n",
    "    fps = 0\n",
    "    first_ecg_date = None \n",
    "    for row in df.to_dicts(): \n",
    "        if row['false_positive']: \n",
    "            if fps == 0: \n",
    "                first_ecg_date = row['ecg_date']\n",
    "            fps += 1\n",
    "            repeat_false_positives.append({\n",
    "                'empi':row['empi'], \n",
    "                'ecg_date':row['ecg_date'],\n",
    "                'first_ecg_date':first_ecg_date,\n",
    "                'fp_count':fps,\n",
    "            })\n",
    "        else: \n",
    "            fps = 0\n",
    "\n",
    "if PER_PATIENT: \n",
    "    repeat_false_positives = pl.from_dicts(repeat_false_positives).sort('fp_count', descending=True).unique(subset='empi', keep='first').to_dicts()\n",
    "\n",
    "\n",
    "N_values = []\n",
    "examples = []\n",
    "any_bad_percent = []\n",
    "days_to_bad = []\n",
    "for N in range(1,50): \n",
    "    N_fps = [x for x in repeat_false_positives if x['fp_count']==N]\n",
    "    if not len(N_fps): continue \n",
    "    any_bad_tally = 0\n",
    "    times_to_first_bad = []\n",
    "    for x in N_fps: \n",
    "        date = x['first_ecg_date']\n",
    "        lvef_after_warning = lvef.filter(pl.col('empi')==x['empi']).filter(pl.col('lvef_date')>=date)\n",
    "        any_bad = not lvef_after_warning.filter(pl.col('lvef')<=40).is_empty()\n",
    "        any_bad_tally += any_bad\n",
    "        time_to_first_bad = None \n",
    "        if any_bad: \n",
    "            time_to_first_bad = lvef_after_warning.filter(pl.col('lvef')<=40).sort('lvef_date').head(1).select('lvef_date').item() - date.date()\n",
    "            # results.append((x['empi'], time_to_first_bad))\n",
    "            times_to_first_bad.append(time_to_first_bad)\n",
    "    \n",
    "    percent = round(100*any_bad_tally/len(N_fps))\n",
    "    days = np.mean(times_to_first_bad).days if times_to_first_bad else 0\n",
    "\n",
    "    # print(f'When we have {N} repeated false positives, there is a low LVEF observed in the future {percent}% of the time.')\n",
    "    # print(f'In these {percent}% cases, the low LVEF is observed within {days} days on average.')\n",
    "    # print()\n",
    "\n",
    "    N_values.append(N)\n",
    "    any_bad_percent.append(percent)\n",
    "    days_to_bad.append(days)\n",
    "    examples.append(len(N_fps))\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6), sharex=False)\n",
    "\n",
    "x_max = max(N_values)+1\n",
    "\n",
    "ax2.plot(N_values, any_bad_percent, marker='o', linestyle='-', color=MY_NAVY, zorder=2)\n",
    "ax2.set_ylim(0, 105)\n",
    "ax2.set_xlim(0, x_max)\n",
    "ax2.set_xticks(range(0, x_max, 5))\n",
    "ax2.set_yticks(range(0, 101, 20))\n",
    "ax2.set_title(\"How often is low LVEF observed in the future?\", fontsize=14)\n",
    "ax2.set_xlabel(\"Subsequent false positive predictions\", fontsize=12)\n",
    "ax2.set_ylabel(\"Percentage\", fontsize=12)\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "ax3.plot(N_values, days_to_bad, marker='o', linestyle='-', color=MY_NAVY, zorder=2)\n",
    "ax3.set_xlim(0, x_max)\n",
    "ax3.set_xticks(range(0, x_max, 5))\n",
    "ax3.set_title(\"Average days until low LVEF is observed\", fontsize=14)\n",
    "ax3.set_xlabel(\"Subsequent false positive predictions\", fontsize=12)\n",
    "ax3.set_ylabel(\"Days\", fontsize=12)\n",
    "ax3.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "\n",
    "ax1.plot(N_values, examples, marker='o', linestyle='-', color=MY_NAVY, zorder=2)\n",
    "ax1.set_ylim(0, max(examples) + 50)\n",
    "ax1.set_xlim(0, x_max)\n",
    "ax1.set_xticks(range(0, x_max, 5))\n",
    "ax1.set_yticks(range(0, max(examples) + 50, 100))\n",
    "ax1.set_title(\"Number of examples for each N-value\", fontsize=14)\n",
    "ax1.set_xlabel(\"Subsequent false positive predictions\", fontsize=12)\n",
    "ax1.set_ylabel(\"Number of patients\" if PER_PATIENT else \"Number of ECGs\", fontsize=12)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(pt): \n",
    "    idx = pyd.data.query('empi==@pt').sort_values('ecg_date').index.values\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Left y-axis: LVEF\n",
    "    # ax1.set_ylabel('LVEF (%)', color='gray')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.set_yticks(range(0, 101, 20))\n",
    "    ax1.tick_params(axis='y', colors='gray')\n",
    "    ax1.spines['left'].set_position(('outward', 10))\n",
    "    ax1.spines['left'].set_color('gray')\n",
    "    ax1.spines['left'].set_linewidth(1)\n",
    "\n",
    "    plt_lvef_date = pd.to_datetime(lvef.filter(pl.col('empi') == pt).select('lvef_date').to_numpy().reshape(-1))\n",
    "    plt_lvef_val = lvef.filter(pl.col('empi') == pt).select('lvef').to_numpy().reshape(-1)\n",
    "    plt_lvef_col = ['silver' if x > 40 else 'firebrick' for x in plt_lvef_val]\n",
    "\n",
    "\n",
    "    for i in range(len(plt_lvef_date)-1): \n",
    "        ax1.plot(plt_lvef_date[i:i+2], plt_lvef_val[i:i+2], c=plt_lvef_col[i+1], zorder=1)  \n",
    "\n",
    "    # ax1.plot(plt_lvef_date, plt_lvef_val, c='silver', zorder=1) \n",
    "    ax1.scatter(plt_lvef_date, plt_lvef_val, marker='o', c=plt_lvef_col, zorder=2)\n",
    "\n",
    "    # Left y-axis threshold\n",
    "    ax1.axhline(y=40, color='firebrick', linestyle='--')\n",
    "\n",
    "    # Configure x-axis\n",
    "    xlim_dates = plt_lvef_date\n",
    "    ax1.set_xlim(left=xlim_dates.min().replace(month=1, day=1),\n",
    "                 right=xlim_dates.max().replace(month=12, day=31, year=max(xlim_dates).year + 1))\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    # ax1.xaxis.set_minor_locator(mdates.MonthLocator(interval=3))\n",
    "    # ax1.xaxis.set_minor_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "    # Set x-axis label and tick colors to gray\n",
    "    ax1.tick_params(axis='x', colors='gray', which='both')\n",
    "    ax1.spines['bottom'].set_color('gray')\n",
    "\n",
    "    ax1.grid(True, axis='x', which='major', linestyle='--', alpha=0.7)\n",
    "    ax1.grid(True, axis='x', which='minor', linestyle=':', alpha=0.5)\n",
    "\n",
    "    # Shared x-axis tick styling\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right', color='gray')\n",
    "    plt.setp(ax1.xaxis.get_minorticklabels(), rotation=45, ha='right', color='gray', fontsize='small')\n",
    "\n",
    "    # Axes styling\n",
    "    ax1.spines['bottom'].set_position(('data', 0))  \n",
    "    ax1.spines['bottom'].set_linewidth(1)\n",
    "\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "\n",
    "    # Right y-axis: Likelihood (0-1)\n",
    "    ax2 = ax1.twinx() \n",
    "    # ax2.set_ylabel('Likelihood (0-1)', color='gray')\n",
    "    ax2.set_ylim(0, 1.005)\n",
    "\n",
    "    ax2.set_yticks([i / 10 for i in range(11)])\n",
    "    ax2.tick_params(axis='y', colors='gray')\n",
    "    ax2.spines['right'].set_position(('outward', 10))\n",
    "    ax2.spines['right'].set_color('gray')\n",
    "    ax2.spines['right'].set_linewidth(1)\n",
    "\n",
    "    dates_ecg = np.array([pyd.data.loc[i, 'ecg_date'].date() for i in idx])\n",
    "    plt_ecg_pred = 1 - pred[idx]\n",
    "    plt_ecg_color = [MY_NAVY if x < (1-threshold) else 'silver' for x in plt_ecg_pred]\n",
    "    ax2.scatter(dates_ecg, plt_ecg_pred, marker='x', zorder=3, color=plt_ecg_color) \n",
    "\n",
    "    # Right y-axis threshold\n",
    "    ax2.axhline(y=1-threshold, color='#001F54', linestyle='--')\n",
    "\n",
    "    # Axes styling\n",
    "    ax2.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "\n",
    "    # Align ticks on both axes\n",
    "    ax1.tick_params(axis='y', direction='in', pad=5)\n",
    "    ax2.tick_params(axis='y', direction='in', pad=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pt in [100202452,100223049,101091462, 103200474, 105812520, 100358399, 100615898, 101137610, 103401384]: \n",
    "    plot_trajectory(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
